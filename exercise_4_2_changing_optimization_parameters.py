# -*- coding: utf-8 -*-
"""Exercise 4.2 Changing optimization parameters.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Darfvsa4gO6NA5FNkMnKAw3ubjr4kLEC

## Exercise 4.2 Changing optimization parameters

In this exercise, you will use the same titanic dataset from the previous exercises. You can download the dataset from the following link:

[titanic_all_numeric.csv](https://drive.google.com/file/d/11nuYS-l3EXCsGJt81y4YTt3oTnFGaB68/view?usp=drive_link)

The data is pre-loaded into a pandas DataFrame called `df`. The `predictors` and `target` values are also pre-defined.

You'll want the optimization to start from scratch every time you change the learning rate, to give a fair comparison of how each learning rate did in your results. So we have created a function `get_new_model()` that creates an unoptimized model to optimize.

It's time to get your hands dirty with optimization. You'll now try optimizing a model at a very low learning rate, a very high learning rate, and a "just right" learning rate. You'll want to look at the results after running this exercise, remembering that a low value for the loss function is good.

## Instructions

* Import `SGD` from `tensorflow.keras.optimizers`.
* Create a list of learning rates to try optimizing with called `lr_to_test`. The learning rates in it should be `0.000001`, `0.01`, and `1.0`.
* Using a `for` loop to iterate over `lr_to_test`:
  * Use the `get_new_model()` function to build a new, unoptimized model.
  * Create an optimizer called `my_optimizer` using the `SGD()` constructor with keyword argument `learning_rate=lr`.
  * Compile your model. Set the optimizer parameter to be the SGD object you created above, and because this is a classification problem, use `'categorical_crossentropy'` for the loss parameter, , and `metrics=['accuracy']` to see the accuracy at the end of each epoch.
  * Fit the model using the `predictors` and the `target`.

## Code

Load data and convert the data to NumPy array:
"""

import numpy as np
import pandas as pd
from tensorflow.keras.utils import to_categorical

# Load csv file into the dataframe: df
df = pd.read_csv("titanic_all_numeric.csv")

# Convert the boolean values of the 'age_was_missing' column to integer
df.age_was_missing = df.age_was_missing.replace({True: 1, False: 0})

# Create predictors NumPy array: predictors
predictors = df.drop(['survived'], axis=1).values

# Save the number of columns in predictors: n_cols
n_cols = predictors.shape[1]

# Convert the target to categorical: target
target = to_categorical(df['survived'])

"""Create a neural network for a classification task"""

from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential

def get_new_model():
  # Set up the model
  model = Sequential()
  model.add(Dense(32, activation='relu', input_shape=(n_cols,)))
  model.add(Dense(2, activation='softmax'))

  return model

"""Optimize the model with different learning rate:"""

# Import the SGD optimizer
from tensorflow.keras.optimizers import SGD

# Create list of learning rates: lr_to_test
lr_to_test = [0.000001, 0.01, 1.0]

# Loop over learning rates
for lr in lr_to_test:
    print('\n\nTesting model with learning rate: %f\n'%lr )

    # Build new model to test, unaffected by previous models
    model = get_new_model()

    # Create SGD optimizer with specified learning rate: my_optimizer
    my_optimizer = SGD(learning_rate=lr)

    # Compile the model
    model.compile(optimizer=my_optimizer,
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    # Fit the model
    model.fit(predictors,
              target,
              epochs=1,
              verbose=1,
              batch_size=32)

"""The ouput should be:

Testing model with learning rate: 0.000001

28/28 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.4074 - loss: 5.5751   


Testing model with learning rate: 0.010000

28/28 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.5747 - loss: 3.3378   


Testing model with learning rate: 1.000000

28/28 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.5404 - loss: 28697.6289

"""