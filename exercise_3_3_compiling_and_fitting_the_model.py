# -*- coding: utf-8 -*-
"""Exercise 3.3 Compiling and Fitting the model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/121QhCN8Gw7Wof7bNo9A1e3_zVtFXZRkF

## Exercise 3.3 Compiling and Fitting the model

In this exercise, you will write code to compile and fit your model.

As the same as the previous exercise, you will work with a .csv file that contains some data about workers in a company. You can download this file from the following link:

[hourly_wages.csv](https://drive.google.com/file/d/1elGBb48CX728knwJzU3ImHNvtuRoG7vu/view?usp=drive_link)

The data is pre-loaded into a pandas DataFrame called `df`. The target variable you'll be predicting is `wage_per_hour`. For convenience, everything in `df` except for the target has been converted to a NumPy array called `predictors`. The target, `wage_per_hour`, is available as a NumPy array called `target`.

You'll create a neural network with two hidden layers and an output layer. The network can be created with the `Sequential` model constructor and the `Dense` layer constructor.

To compile the model, you need to specify the optimizer and loss function to use. In general, Adam optimizer is an excellent choice so you'll use the Adam optimizer and the mean squared error loss function.

After successfully compiling the model, you can fit your model using the `predictors` data and the `target` data.

## Instructions

* Create you model with the same code in the previous exercise
* Compile the model using `model.compile()`. Your optimizer should be `'adam'` and the loss should be `'mean_squared_error'`.
* Fit the model using `model.fit()`. Remember that the first argument is the predictive features (`predictors`), and the data to be predicted (`target`) is the second argument.

## Code

Load data and convert the data to NumPy array:
"""

import pandas as pd
# Load csv file into the dataframe: df
df = pd.read_csv("hourly_wages.csv")
# Split the dataframe df into two dataframes:
wagePerHourDf = df.iloc[:,0]
predictorsDf = df.iloc[:,1:df.shape[1]]

# Create predictors NumPy array: predictors
predictors = predictorsDf.to_numpy()

# Create target NumPy array: target
target = wagePerHourDf.to_numpy()

"""# New section

Create the neural network, then compile and fit the model
"""

# Import necessary modules
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential

# Save the number of columns in predictors: n_cols
n_cols = predictors.shape[1]

# Set up the model: model
model = Sequential()

# Add the first layer
model.add(Dense(50, activation='relu', input_shape=(n_cols,)))

# Add the second layer
model.add(Dense(32, activation='relu'))

# Add the output layer
model.add(Dense(1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Fit the model
model.fit(predictors, target, epochs=10, batch_size=32)

"""The ouput should be similar to the following:


17/17 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - loss: 24.1951   
<keras.src.callbacks.history.History at 0x7f06186959f0>


You don't have to care much about the meaning of the output. The detail will be explained soon. At this time, just think of it as a log showing model performance on the training data as we update model weights.

"""